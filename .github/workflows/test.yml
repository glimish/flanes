name: Tests

on:
  push:
    branches: [main]
  pull_request:

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: pip install -e ".[dev]"

      - name: Lint with ruff
        run: ruff check flanes/ tests/

      - name: Check formatting with ruff
        run: ruff format --check flanes/ tests/

      - name: Type check with mypy
        run: mypy flanes/
        continue-on-error: true  # Pre-existing issues; tracked for gradual fix

  test-ubuntu:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.10", "3.11", "3.12", "3.13"]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: pip install -e ".[dev]"

      - name: Run fast tests
        run: python -X utf8 -m pytest tests/ -v -m "not stress and not slow"
        timeout-minutes: 10

      - name: Run slow tests
        run: python -X utf8 -m pytest tests/ -v -m "stress or slow"
        timeout-minutes: 15

      - name: Run example smoke test
        run: python -X utf8 examples/agent_workflow.py
        timeout-minutes: 5

  test-windows:
    runs-on: windows-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.10", "3.11", "3.12", "3.13"]

    # Windows CI runners deliver a spurious KeyboardInterrupt during Python
    # interpreter shutdown, causing pytest to exit with code 1 even when all
    # tests pass.  We write JUnit XML results and verify them in a final step.
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: pip install -e ".[dev]"

      - name: Run fast tests
        id: fast
        run: python -X utf8 -m pytest tests/ -v -m "not stress and not slow" --junitxml=fast-results.xml
        continue-on-error: true
        timeout-minutes: 10

      - name: Run slow tests
        id: slow
        run: python -X utf8 -m pytest tests/ -v -m "stress or slow" --junitxml=slow-results.xml
        continue-on-error: true
        timeout-minutes: 15

      - name: Run example smoke test
        run: python -X utf8 examples/agent_workflow.py
        continue-on-error: true
        timeout-minutes: 5

      - name: Verify test results
        if: always()
        shell: python
        run: |
          import xml.etree.ElementTree as ET, sys, os
          all_ok = True
          total_tests = 0
          for xml_file in ["fast-results.xml", "slow-results.xml"]:
              if not os.path.exists(xml_file):
                  print(f"WARN: {xml_file} not found")
                  continue
              root = ET.parse(xml_file).getroot()
              # pytest writes <testsuites><testsuite .../></testsuites>
              suites = root.findall("testsuite") if root.tag == "testsuites" else [root]
              for suite in suites:
                  failures = int(suite.get("failures", 0))
                  errors = int(suite.get("errors", 0))
                  tests = int(suite.get("tests", 0))
                  skipped = int(suite.get("skipped", 0))
                  total_tests += tests
                  print(f"{xml_file}: {tests} tests, {failures} failures, {errors} errors, {skipped} skipped")
                  if failures > 0 or errors > 0:
                      all_ok = False
          if total_tests == 0:
              print("ERROR: No test results found")
              sys.exit(1)
          if not all_ok:
              print("REAL test failures detected")
              sys.exit(1)
          print(f"All {total_tests} tests passed (any non-zero pytest exit was spurious)")

  benchmark:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    continue-on-error: true

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: pip install -e ".[dev]"

      - name: Benchmark snapshots
        run: python -X utf8 -m benchmarks.bench_snapshot --files 1000 --rounds 3 --json

      - name: Benchmark throughput
        run: python -X utf8 -m benchmarks.bench_throughput --scenario snapshot --files 100 500 1000 --json
